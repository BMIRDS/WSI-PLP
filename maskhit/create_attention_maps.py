# Python standard libraries
import argparse
import glob
import os
import pickle

# Third-party libraries
import cv2
import matplotlib.patches as mp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from skimage.morphology import remove_small_holes
from tqdm import tqdm

# Own libraries
from utils.config import Config
from vis.wsi import WSI

"""
This script should be run after obtaining attention scores for the pre-trained model and fine-tuned model.

This script allows a user to visualize attention maps generated from the pre-trained and fine-tuned model.
    - Processes metadata about WSIs and model checkpoints to restore aggregated
    attention maps from pickle files generated by previous scripts
    - Computes attention maps showing the differences between the pre-trained and fine-tuned models
    - Allows the detailed inspection of attention at varying levels of magnification (10x, 20x, 40x)
    - Saving visualizations in output directory

Command line arguments
    - default-config-file and user-config-file
    - please see default-config-file for information on how to set up the file
"""

class WSIVisualizer(WSI):
    def __init__(self, meta_dict: dict, svs_id: int, id_ft: int, id_pt: int, 
                 mag_mask: float = 1.25, config_file: str = None, complete_pipeline: bool = False):

        """
        Args:
        - meta_dict (dict): Dictionary consisting of model checkpoints
        - svs_id (int): Identifier for the WSI to be analyzed
        - id_ft (int): Index of the fine-tuned model in the meta_dict
        - id_pt (int): Index of the pre-trained model in the meta_dict
        - mag_mask (float): Magnification level for the mask. Default is 1.25
        - config_file (str): Path to the user-defined configuration file. Default is None
        - complete_pipeline (bool): If True, runs the complete pipeline for visualizing the attention overlay
        """

        self.df_meta_ckp = pd.DataFrame(meta_dict).sort_values('order').reset_index(drop=True)
        self.id_ft = id_ft
        self.id_pt = id_pt
        self.id_to_path = {} # dictionary to store svs_id to svs_path mapping
        self.svs_id = svs_id
        self.mag_mask = mag_mask
        self.masks = {}
        self.process_meta() # process metadata to fill up id_to_path dictionary
    
        if config_file:
            self.config_file = config_file
            self.old_base_path = config_file.dataset.old_base_path
            self.new_base_path = config_file.dataset.new_base_path
        else:
            self.old_base_path = self.new_base_path = None
        
        # create output directory if it does not exist
        os.makedirs("output", exist_ok=True)

        # Extract the svs path associated with svs_id
        self.svs_path = self.id_to_path[svs_id]
        print("Running visualizations with ", (self.svs_path, svs_id))

        if self.new_base_path:
            self.svs_path = self.svs_path.replace(self.old_base_path, self.new_base_path)
        self.wsi = WSI(self.svs_path) # create WSI object with the specified svs_path

        # check for pressence of features_agg directory
        if not os.path.exists('features_agg'):
            
            # obtaining attention maps for the specified model
            for i, row in self.df_meta_ckp.iterrows():
                print(i, row)
                study, model_name, epoch, svs_dir = row['study'], row['model_name'], row['epoch'], row['svs_dir']
                print('<>'*30)
                print(model_name, study, epoch, svs_dir)
                self.compress_global_attention(model_name, epoch, plot_result=False)
        
        if complete_pipeline:
            # running the complete pipeline for visualizing the attention overlay
            _, _ = self.complete_pipeline(sel_region=(300,242,300,1.25), 
                      sel_patch=[(300,242,250,40)], 
                      cl=0.25, ch=0.75)
            print("FINISHED RUNNING COMPLETE_PIPELINE")
    
    def process_meta(self, svs_dir: str = 'svs_2019'):
        """
        Reads the metadata and processes it to fill up the id_to_path dictionary,
        which maps ids to their paths
        """
        
        layer_i = head_i = 'None'
        for _, row in self.df_meta_ckp.iterrows():
            study = row['study']
            model_name = row['model_name']
            epoch = row['epoch']

            try:
                df_meta = pd.read_pickle(f'features/{model_name}/{layer_i}-{head_i}/{epoch}/meta.pickle')
            except Exception as e:
                print(e)
                continue
            
            if 'svs_path' in df_meta.columns:
                self.id_to_path = df_meta.drop_duplicates('id_svs').set_index('id_svs_num')['svs_path'].to_dict()
                continue
            
            # will not reach this point if the svs_path column is already present
            for _, row_i in df_meta.drop_duplicates('id_svs_num').iterrows():
                svs_id = row_i['id_svs']
                file_id = df_meta.loc[df_meta.id_svs == svs_id].id_svs.tolist()[0]
                svs_path = glob.glob(f"/datasets/{self.config_file.dataset.wsi_name}/{svs_dir}/{file_id}.svs")[0]
                self.id_to_path[svs_id] = svs_path
            
            svs_path_mapping = pd.DataFrame([], columns=['id_svs','svs_path'])
            df_meta = df_meta.merge(svs_path_mapping, on='id_svs')
            df_meta.to_pickle(f'features/{model_name}/{layer_i}-{head_i}/{study}/{epoch}/meta.pickle')
    
    def restore_agg_attn_map(self, row: pd.Series):
        """
        Parameters:
        - row (pd.Series): expected to have following keys: model_name, study, and epoch

        Returns:
        - attn_maps (dict): Dictionary containing attention maps, loaded from a pickle file.
        These attention maps were generated using the compress_global_attention function.

        Note: The function assumes the existence of a features_agg directory with a specified directory structure
        """

        model_name, study, epoch = row['model_name'], row['study'], row['epoch']
        try:
            # with study name as part of path to attention map file
            with open(f"features_agg/{model_name}/None-None/{study}/{epoch}/attn_map.pickle", "rb") as f:
                attn_maps = pickle.load(f)
        except FileNotFoundError:
            try:
                # without study name as part of path to attention map file
                with open(f"features_agg/{model_name}/None-None/{epoch}/attn_map.pickle", "rb") as f:
                    attn_maps = pickle.load(f)
            except FileNotFoundError as e:
                print(f"Error: {e}")
        return attn_maps

    def get_diff_map(self, id_svs: str):
        """
        Compute and visualize difference, fine-tuned (ft), and pre-trained (pt) attention maps

        Parameters:
        - id_svs (str): Identifier for the WSI to be analyzed.

        Returns:
        - diff_map: Difference between the fine-tuned and pre-trained model attention maps.
        - attn_maps_ft[id_svs]: Attention map for the fine-tuned model on the specified WSI.
        - attn_maps_pt[id_svs]: Attention map for the pre-trained model on the specified WSI.
        - name (str): Location where the plot was saved
        - study (str): Study name related to the fine-tuned model.

        Note: Creates a subplot of the attention maps for the fine-tuned and pre-trained model. Also includes
        the map of the difference between the two. Saves these plots in the output directory.
        """
        
        # restoring attention maps from fine-tuned model
        row_ft = self.df_meta_ckp.iloc[self.id_ft]
        attn_maps_ft = self.restore_agg_attn_map(row_ft)
        study_name = row_ft['study']
        
        # restoring attention maps from pre-trained model
        row_pt = self.df_meta_ckp.iloc[self.id_pt]
        attn_maps_pt = self.restore_agg_attn_map(row_pt)
        
        map_name = f"ft{row_ft['model_name']}-pt{row_pt['model_name']}"
        fig, axes = plt.subplots(1,3, figsize=(10,15))
        id_svs = str(id_svs) # convert to string for index purposes

        attn_maps_diff = attn_maps_ft[id_svs] - attn_maps_pt[id_svs] # # computing the difference map

        # max_value for normalization across all attention maps
        max_value = max(np.abs(attn_maps_ft[id_svs]).max(), np.abs(attn_maps_pt[id_svs]).max(), np.abs(attn_maps_diff).max())
        
        # Fine-tuned Attention Map
        ft_map = axes[0].imshow(attn_maps_ft[id_svs], cmap='coolwarm', vmin=0.5, vmax=1.5)
        axes[0].axis('off')
        axes[0].set_title('Fine-tuned Attention Map')
        fig.colorbar(ft_map, ax=axes[0], orientation='vertical', fraction=0.046, pad=0.04)

        # Pre-trained Attention Map
        pt_map = axes[1].imshow(attn_maps_pt[id_svs], cmap='coolwarm', vmin=0.5, vmax=1.5)
        axes[1].axis('off')
        axes[1].set_title('Pre-trained Attention Map')
        fig.colorbar(pt_map, ax=axes[1], orientation='vertical', fraction=0.046, pad=0.04)

        # Difference Map
        diff_map = axes[2].imshow(attn_maps_diff, cmap='coolwarm', vmin=-max_value, vmax=max_value)
        axes[2].axis('off')
        axes[2].set_title('Difference Map')
        fig.colorbar(diff_map, ax=axes[2], orientation='vertical', fraction=0.046, pad=0.04)

        # Save the figure
        save_path = f"output/{map_name}"
        plt.savefig(save_path, bbox_inches='tight')
        print(f"Attention Map Subplots saved at {save_path}")

        return attn_maps_diff, attn_maps_ft[id_svs], attn_maps_pt[id_svs], map_name, study_name

    def plot_local_attention(self, attn_map3: np.ndarray, vmin, vmax, 
                             pos_x: int, pos_y: int, region_size: int, magnification: float, 
                             annotation: list = None, save_fig: bool = False, save_dir: str = "", label: str =""):
        """
        Plots local attention map of a specified region
        
        Parameters:
        - attn_map3 (numpy.ndarray): 2D array representing the attention map.
        - vmin, vmax
        - pos_x (int): X-coordinate of the top-left corner of region to be visualized.
        - pos_y (int): Y-coordinate of the top-left corner of region to be visualized.
        - region_size (int): Size of the region to be visualized.
        - magnification (float): magnification of WSI.
        - annotation (list): List of tuples (x, y, size). Default is None
        - save_fig (bool): If True, save the figures as SVG
        - save_dir (str): Directory path to save the figures

        Returns: None
        """

        print(f"Magnification in plot_local_attention function: {magnification}")

        # extracting specific region from the attention map
        attn_clip = attn_map3[(max(0, pos_y)):(pos_y+region_size), (max(0,pos_x)):(pos_x+region_size)]
        region_name = f"r_{region_size}-m_{str(magnification)}-{pos_x}_{pos_y}"
        tile = self.wsi.get_region(pos_x, pos_y, int(region_size*magnification/1.25), magnification, 1.25)

        # plotting selected tile extracted based on pos_x and pos_y
        tile_height, tile_width = tile.shape[:2]
        print(f"Tile Height and Width: ({tile_height}, {tile_width})")
        print(f"Plot Size (Tile): ({tile_width / 100}, {tile_height / 100})")

        # plotting the tile only
        fig = plt.figure(figsize=(tile_width / 100, tile_height / 100))
        plt.imshow(tile, aspect='equal')
        plt.axis('off')
        if save_fig:
            save_loc = save_dir + f"{label}-{region_name}-tile.svg"
            plt.savefig(save_loc, bbox_inches='tight',pad_inches = 0)
            print(f"Saved tile figure at location: {save_loc}")
            plt.close()

        # plotting a specific region of attention map overlayed with tile
        print(f"Plot Size (Attn_Map): ({tile_width / 100}, {tile_height / 100})")
        fig = plt.figure(figsize=(tile_width / 100, tile_height / 100))
        ax1 = fig.add_axes((0,0,1,1), label='wsi')
        ax2 = fig.add_axes((0,0,1,1), label='attn')
        ax1.imshow(attn_clip, vmin=vmin, vmax=vmax, alpha=0.6, cmap='coolwarm')
        ax1.axis('off')        
        ax2.imshow(tile, alpha=0.5)
        ax2.axis('off')

        # adding annotations (rectangle) to the image
        if annotation is None:
            pass
        else:
            for pos in annotation:
                pos_x, pos_y, region_size = pos
                rect = mp.Rectangle((pos_x,pos_y), region_size, region_size, 
                                    linewidth=2, edgecolor='black', facecolor='none')
                ax2.add_patch(rect)     
        
        # saving the overlay (if specified)
        if save_fig:
            save_loc = save_dir + f"{label}-{region_name}-overlay.svg"
            plt.savefig(save_loc,bbox_inches='tight',pad_inches = 0)
            print(f"Saved figure at location: {save_loc}")
            plt.close()

    def get_local_attention_overlay(self, save_dir: str, attn_map3: np.ndarray, vmin, vmax, 
                                    sel_region: tuple, sel_patch: list, save_fig: bool = False, label: str = ""):
        """
        Repetitively plots local attention maps based on specified patch locations

        Parameters:
        - save_dir (str): Directory path where the figures will be saved, if `save_fig` is True.
        - attn_map3 (numpy.ndarray): 2D array representing the attention map.
        - vmin, vmax
        - sel_region (tuple): tuple with following structure (pos_x, pos_y, region_size, magnification).
        - sel_patch (list): List of tuples with format: (pos_x, pos_y, region_size, magnification).
        - save_fig (bool): If True, save the figures in the specified `save_dir`. Default is False.
        """     

        # plotting the attention map with the selected region and patches in rectangles
        fig, ax = plt.subplots(1,1,figsize=(8,8))
        ax.imshow(attn_map3)
        for sel_i in [sel_region]+sel_patch:
            pos_x, pos_y, region_size, _ = sel_i
            rect = mp.Rectangle((pos_x,pos_y), region_size, region_size, linewidth=2, edgecolor='blue', facecolor='none')
            ax.add_patch(rect)
        plt.show()
        
        # plotting the region
        pos_x, pos_y, region_size, magnification = sel_region
        annotation = []
        for sel_i in sel_patch:
            pos_xp, pos_yp, region_size_p, _ = sel_i
            annotation.append([pos_xp-pos_x, pos_yp-pos_y,region_size_p])
        self.plot_local_attention(attn_map3, vmin, vmax, pos_x, pos_y, region_size, magnification, annotation, save_fig=save_fig, save_dir=save_dir, label=label)
        
        # plotting the patches
        for i, sel_i in enumerate(sel_patch):
            pos_xp, pos_yp, region_size, magnification = sel_i
            self.plot_local_attention(attn_map3, vmin, vmax, pos_xp, pos_yp, region_size, magnification, save_fig=save_fig, save_dir=save_dir, label=label)


    def complete_pipeline(self, save_fig: bool = True, cl: float = 0.2, ch: float = 0.8, sel_region: 
                          tuple = None, sel_patch: list = None, alpha: float =0.1):
        """
        Complete pipeline for visualizing the attention overlay 
        on whole slide images, based on the attention map differences.

        Parameters:
        - save_fig (bool): Flag indicating whether to save figures. Default is True.
        - cl, ch
        - sel_region (tuple): tuple of the form (pos_x, pos_y, region_size, magnification). Default is None.
        - sel_patch (list, optional): List of tuples of the form 
        (pos_x, pos_y, region_size, magnification). Default is None.
        - alpha (float, optional): Transparency level for overlays on top of thumbnails. Default is 0.3.

        Returns:
        - attn_map_fine (numpy.ndarray): 2D fine-tuned attention map array for the selected WSI.
        - attn_map_diff (numpy.ndarray): 2D difference attention map array for the selected WSI.
        """

        # generating the thumbnail for the specified WSI
        thumbnail = self.wsi.downsample(self.mag_mask)
        thumbnail2, (max_x, max_y), (new_x, new_y) = WSI.crop_prop_img(thumbnail)

        if self.wsi.svs_path not in self.masks.keys():
            mask = WSI.filter_purple(thumbnail2)
            self.masks[self.wsi.svs_path] = remove_small_holes(mask==1,400)
        mask = self.masks[self.wsi.svs_path] # obtaining the mask for the specified WSI

        # getting the attention maps for the specified WSI
        diff_map, fine_map, pre_trained_map, map_name, study = self.get_diff_map(self.svs_id)

        id_patient = self.wsi.svs_path.split('/')[-1][:12] # dependent on the naming convention of the WSI
        save_dir = f"output/visualization/{study}/{id_patient}/{map_name}/"
        os.makedirs(save_dir, exist_ok=True)

        # Smoothening the attention maps
        attn_map_fine = self.smooth_attn_map(fine_map, new_x, new_y)
        attn_map_diff = self.smooth_attn_map(diff_map, new_x, new_y)

        # Processing the attention maps
        attn_map_fine = self.process_map(attn_map_fine, mask)
        attn_map_diff = self.process_map(attn_map_diff, mask)

        # obtaining quantile values for fine-tuned and difference attention map
        vmin_fine, vmax_fine = np.nanquantile(attn_map_fine, cl),np.nanquantile(attn_map_fine, ch)
        vmin_diff, vmax_diff = np.nanquantile(attn_map_diff, cl),np.nanquantile(attn_map_diff, ch)

        self.plot_attn_map(attn_map_fine, vmin_fine, vmax_fine, thumbnail2, sel_region, save_dir, 'fine', save_fig, sel_patch)
        self.plot_attn_map(attn_map_diff, vmin_diff, vmax_diff, thumbnail2, sel_region, save_dir, 'diff', save_fig, sel_patch)

        return attn_map_fine, attn_map_diff

    def smooth_attn_map(self, attn_map, new_x, new_y):
        """
        Applies guassian blur to smooth the attention map

        Args:
            - attn_map (np.ndarray): 2D array representing the attention map
            - new_x (int), new_y (int): to resize the attention map
        
        Important Note: Filters for Gaussian Blur is applied at stride (3, 3). This 
        may need to be adjusted depending on the quality of the visualization.
        """

        attn_map = np.nan_to_num(attn_map)
        smoothed_attn_map = cv2.GaussianBlur(attn_map, (3, 3), 0) # smoothing the attention map
        attn_map = np.nan_to_num(smoothed_attn_map)

        attn_map = cv2.resize(attn_map, (new_y, new_x), interpolation=cv2.INTER_LINEAR)
        
        return attn_map

    def process_map(self, attn_map, mask):
        """
        Returns a processed attention map that aligns with the WSI mask

        Args:
            - attn_map (np.ndarray): 2D array representing the attention map
            - mask (np.ndarray): 2D array representing the mask

        Returns:
            - attn_map3 (np.ndarray): 2D array representing the processed attention map
        """

        attn_map2 = attn_map.copy()
        attn_map2[np.isnan(attn_map2)] = 0
        attn_map3 = mask*attn_map2
        attn_map3[attn_map3 == 0] = np.nan 

        return attn_map3

    def plot_attn_map(self, attn_map, vmin, vmax, thumbnail2, sel_region, save_dir, label, save_fig, sel_patch):
        """
        This function plots the attention map, the thumbnail, and the combined plot

        Args:
            - attn_map (np.ndarray): 2D array representing the attention map
            - vmin, vmax
            - thumbnail2 (np.ndarray): 2D array representing the thumbnail
            - sel_region (tuple): tuple of the form (pos_x, pos_y, region_size, magnification)
            - save_dir (str): Directory path to save the figures
            - label (str): Label for saving the figure
            - save_fig (bool): If True, save the figures
            - sel_patch (list): List of tuples of the form (pos_x, pos_y, region_size, magnification)
        """
        
        # setting up the plot
        fig = plt.figure(figsize=(10,10))
        ax1 = fig.add_axes((0,0,1,1), label='wsi')
        ax2 = fig.add_axes((0,0,1,1), label='attn')  

        # attention map plot
        im = ax1.imshow(attn_map, alpha=1.0, vmin=vmin, vmax=vmax, cmap='coolwarm')
        ax1.axis('off')
        
        # original thumbnail plot
        ax2.imshow(thumbnail2, alpha=0.5)
        ax2.axis('off')

        # drawing a rectangle around the selected region to visualize
        if sel_region is not None:
            pos_x, pos_y, region_size, _ = sel_region
            rect = mp.Rectangle((pos_x,pos_y), region_size, region_size, 
                                linewidth=2, edgecolor='black', facecolor='none')
            ax2.add_patch(rect)
        
        # saving the combined plot
        if save_fig:
            save_loc = save_dir+label+'-'+'overview-combined.svg'
            plt.savefig(save_loc, bbox_inches='tight',pad_inches = 0)
            print(f"overview-combined.svg saved at location: {save_loc}")
            plt.close()

        # saving the thumbnail plot
        fig = plt.figure(figsize=(10,10))
        plt.imshow(thumbnail2)
        plt.axis('off')
        if save_fig:
            save_loc = save_dir+label+'-'+'overview-thumbnail.svg'
            plt.savefig(save_loc, bbox_inches='tight',pad_inches = 0)
            print(f"overview-thumbnail.svg saved at location: {save_loc}")
            plt.close()

        # saving the attention map plot
        fig = plt.figure(figsize=(10,10))
        im = plt.imshow(attn_map, vmin=vmin, vmax=vmax, cmap='coolwarm')
        plt.colorbar(im, orientation='vertical', shrink=0.75)  # adding colorbar
        plt.axis('off')
        if save_fig:
            save_loc = save_dir+label+'-'+'overview-heatmap.svg'
            plt.savefig(save_dir+'overview-heatmap.svg',bbox_inches='tight',pad_inches = 0)
            print(f"overview-heatmap.svg saved at location: {save_loc}")
            plt.close()
        
        if sel_region is not None and sel_patch is not None:
            self.get_local_attention_overlay(save_dir, attn_map, vmin, 
                                             vmax, sel_region=sel_region, sel_patch=sel_patch, save_fig=save_fig, label = label)

    def get_attention_map(self, attn_map: np.ndarray, patch_i: int = -1) -> np.ndarray:
        """
        Important Note: This function assumes a grid size of 10x10. In other words, each patch is represented
        in the attention map with 100 values. The .view(10, 10) needs to be adjusted for your specific use case,
        which depends on region size and patch size. If you are still receiving an error, please check 
        the number of patches (num_patches) during extraction of the features. 
        """

        mask = attn_map[patch_i+1,1:].view(10,10)
        n = (mask > 0).sum()
        return mask*n

    def compress_global_attention(self, model_name: str, epoch: int, 
                                  layers_i = 'None', head_i = 'None', plot_result: bool = False) -> None:
        '''
        Saves plots of attention maps of all WSIs for a given model.
        Args:
        - model_name (str): Name of the model
        - epoch (int): Epoch number
        - layers_i (str): Layer index. Default is 'None'
        - head_i (str): Head index. Default is 'None'
        - plot_result (bool): If True, saves the plots. Default is False
        '''

        df = pd.read_pickle(f"features/{model_name}/{layers_i}-{head_i}/{epoch}/meta.pickle")
        df.drop_duplicates('id_svs', inplace=True)

        attn_maps = {}
        cnts_maps = {}
        dims_maps = {}
        thumbnails = {}
        pattern = f"features/{model_name}/{layers_i}-{head_i}/{epoch}/*.pickle"
        files = glob.glob(pattern)
        for fname in tqdm(files):
            # skip meta.pickle file
            if os.path.basename(fname) == 'meta.pickle':
                continue
            with open(fname, 'rb') as f:
                batch_inputs = pickle.load(f)
            n_samples = batch_inputs['ids'].size(0) # batch size in 'extract' mode
            for i in range(n_samples):
                # (x, y) coordinates of the patch (top-left corner of the region in the WSI image)
                pos_x, pos_y = batch_inputs['pos_tile'][i].squeeze().tolist()
                svs_id = str(batch_inputs['ids'][i].item())

                if svs_id not in attn_maps.keys():
                    svs_path = df.loc[df.id_svs_num == int(svs_id), 'svs_path'].item()                    
                    if self.new_base_path:
                        svs_path = svs_path.replace(self.old_base_path, self.new_base_path)
                    
                    # reconstructing WSI object for specified svs_path
                    wsi = WSI(svs_path)
                    thumbnail = wsi.downsample(self.mag_mask)
                    thumbnail, (max_x, max_y), (new_x, new_y) = WSI.crop_prop_img(thumbnail)
                    
                    # dependent on grid size of 10x10 
                    attn_maps[svs_id] = np.zeros((max_x+10, max_y+10))
                    cnts_maps[svs_id] = np.zeros((max_x+10, max_y+10))                    
                    dims_maps[svs_id] = (max_x, max_y)
                    thumbnails[svs_id] = thumbnail
                
                # obtaining attention map for the grid based on the patch index
                attn_map_i = self.get_attention_map(batch_inputs['attn'][i], -1).numpy()
                
                # updating attention map for the specified svs_id at the specified position (dependent on grid size of 10x10)
                attn_maps[svs_id][pos_x:(pos_x+10),pos_y:(pos_y+10)] += attn_map_i
                cnts_maps[svs_id][pos_x:(pos_x+10),pos_y:(pos_y+10)] += attn_map_i != 0

        for svs_id in list(attn_maps.keys()):
            max_x, max_y = dims_maps[svs_id]

            # normalizing attention map by the count of non-zero elements
            attn_maps[svs_id] = attn_maps[svs_id]/cnts_maps[svs_id]
            attn_maps[svs_id] = attn_maps[svs_id][:max_x, :max_y]
            attn_maps[svs_id] = attn_maps[svs_id].swapaxes(1,0)
            
            if plot_result:
                fig = plt.figure(figsize=(8,8))
                axis_1 = fig.add_axes([0,0,1,1], label='1')
                axis_2 = fig.add_axes([0,0,1,1], label='2')

                # displaying the thumbnail
                axis_1.imshow(thumbnails[svs_id])
                axis_1.axis('off')

                # displaying the attention map
                axis_2.imshow(attn_maps[svs_id], alpha=0.3, cmap='viridis')
                axis_2.axis('off')

                save_path = f"{model_name} {svs_id}.png"
                plt.savefig(save_path)
                print(f"Saved attention maps at {save_path}")
                plt.close(fig)

        save_loc = f"features_agg/{model_name}/{layers_i}-{head_i}/{epoch}/"
        os.makedirs(save_loc, exist_ok=True)

        # saving all attention maps in a pickle file for the specified model
        with open(save_loc+'attn_map.pickle', 'wb') as f:
            pickle.dump(attn_maps, f)

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser(
        description='visualization script')
    parser.add_argument(
        "--default-config-file", 
        type=str,
        default='configs/config_default_visualization.yml',
        help="Path to the base configuration file. Defaults to 'config.yaml'.")
    parser.add_argument(
        "--user-config-file", 
        type=str,
        default = 'configs/config_ibd_visualization.yml',
        help="Path to the user-defined configuration file.")
    args = parser.parse_args()
    
    config = Config(args.default_config_file, args.user_config_file)

    # metadata for the pre-trained and fine-tuned models (needs to be updated based on the model checkpoints)
    meta_dict = [
        {
            "order": 0,
            "name": "IBD_pretrained",
            "study": 'IBD_PROJECT',
            "model_name": "-20240205_015916-3",
            "epoch": "0006",
            "svs_dir": "svs_2019"
        },

        {
            "order": 1,
            "name": "IBD_finetuned",
            "study": 'IBD_PROJECT',
            "model_name": "-pretrained_10x_224_resnet18",
            "epoch": "0500",
            "svs_dir": "svs_2019"
        }
    ]

    wsi_visual = WSIVisualizer(meta_dict, svs_id=5, id_ft=1, id_pt=0, 
                               config_file = config, complete_pipeline=True)
    